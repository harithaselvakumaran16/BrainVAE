{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f7fff7",
   "metadata": {},
   "source": [
    "# **Week 2(7/27-8/2)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20aa513",
   "metadata": {},
   "source": [
    "Completed tasks: (9 hours)  \n",
    "1 process data (1 hours)   \n",
    "1.1 divided dataset into a balanced train(80%)/validation(10%)/test(10%) dataset based on \"SA1.35\" attribute. And saved splited result as \"ids_split.pkl\"   \n",
    "1.2 generated brain connectivity maxtrix as the dataset of our project. And saved train/validation/test dataset as \"con_mat_train.npy\",\"con_mat_test.npy\",\"con_mat_val.npy\" \n",
    "1.3 generated brain connectivity maxtrix with all 853 subjects and saved it as \"con_mat_all.npy\" \n",
    "\n",
    "835 subects\n",
    "\n",
    "\n",
    "2 Revise the code (8 hours)  \n",
    "2.1 revised dataset class for brain connectivity matrix   \n",
    "2.2 revised model constructure for our dataset  \n",
    "2.3 revised loss function method  \n",
    "2.4 revised training python file   \n",
    "I borrowed codes from https://github.com/libilab/rsfMRI-VAE and https://github.com/rtqichen/beta-tcvae.  \n",
    "To train a model:  \n",
    "      python vae_quant.py --dataset brain --num_workers 4 --latent-dim 10 --batch-size 16 --num-epochs 150 --beta 15 --conv --tcvae --gpu 0 --save tcvae_10_15 --model_name tcvae_10_15  \n",
    "   \n",
    "We can modify beta and latent-dim values to search optimal parameters.\n",
    "\n",
    "Questions and plans:  \n",
    "1 how to upload codes to discovery conviniently after modifying? Now, every time, I delete a file on Discovery and then upload it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47868cf6",
   "metadata": {},
   "source": [
    "# report details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1ce74",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f73ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce821b76-b317-4704-99e9-e2f518ee6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31885cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_attr = pd.read_spss(r\"C:\\Users\\Administrator\\Desktop\\vae\\data\\AOMIC_Political.sav\").astype('str')\n",
    "roi = pd.read_pickle(r\"C:\\Users\\Administrator\\Desktop\\vae\\data\\TimeData_aal_116.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18517660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_roi = set(roi.keys())       \n",
    "ids_sub_attr = set(sub_attr['ID1000ID'].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b65ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_intersection = list(ids_roi.intersection(ids_sub_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ef658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save ids (totally 853)\n",
    "with open(\"ids_inter.pkl\", 'wb') as text:\n",
    "    pickle.dump(ids_intersection, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9558520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#853 sub_attr_new\n",
    "sub_attr['ID1000ID'] = sub_attr['ID1000ID'].astype('int')\n",
    "sub_attr_new = sub_attr[sub_attr['ID1000ID'].isin(ids_intersection)]  \n",
    "sub_attr_new = sub_attr_new.replace('nan','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4691c1",
   "metadata": {},
   "source": [
    "split data into train(80%), val(10), test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a84bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stratified_into_train_val_test(df_input, stratify_colname='y',\n",
    "                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,\n",
    "                                         random_state=None):\n",
    "    '''\n",
    "    Splits a dataframe into three subsets (train, val, and test)\n",
    "    following fractional ratios provided by the user, where each subset is\n",
    "    stratified by the values in a specific column (that is, each subset has\n",
    "    the same relative frequency of the values in the column).\n",
    "    '''\n",
    "\n",
    "    if frac_train + frac_val + frac_test != 1.0:\n",
    "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
    "                         (frac_train, frac_val, frac_test))\n",
    "\n",
    "    if stratify_colname not in df_input.columns:\n",
    "        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n",
    "\n",
    "    X = df_input # Contains all columns.\n",
    "    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n",
    "\n",
    "    # Split original dataframe into train and temp dataframes.\n",
    "    df_train, df_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                          y,\n",
    "                                                          stratify=y,\n",
    "                                                          test_size=(1.0 - frac_train),\n",
    "                                                          random_state=random_state)\n",
    "\n",
    "    # Split the temp dataframe into val and test dataframes.\n",
    "    relative_frac_test = frac_test / (frac_val + frac_test)\n",
    "    df_val, df_test, y_val, y_test = train_test_split(df_temp,\n",
    "                                                      y_temp,\n",
    "                                                      stratify=y_temp,\n",
    "                                                      test_size=relative_frac_test,\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9137561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = split_stratified_into_train_val_test(sub_attr_new, stratify_colname='SA1.35', frac_train=0.80, frac_val=0.10, frac_test=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecac7f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           576\n",
       "Links       57\n",
       " Rechts     49\n",
       "Name: SA1.35, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['SA1.35'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fcd3b71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           72\n",
       "Links       7\n",
       " Rechts     6\n",
       "Name: SA1.35, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['SA1.35'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bbce3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           72\n",
       " Rechts     7\n",
       "Links       7\n",
       "Name: SA1.35, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['SA1.35'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d3b7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_split = {\n",
    "    'train' : list(df_train['ID1000ID']),\n",
    "    'val' : list(df_val['ID1000ID']),\n",
    "    'test' : list(df_test['ID1000ID']),\n",
    "            }\n",
    "with open(\"ids_split.pkl\", 'wb') as text:\n",
    "    pickle.dump(ids_split, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c199655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"ids_split.pkl\", 'rb') as text:\n",
    "#    ids_split=pickle.load(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54abfb6f",
   "metadata": {},
   "source": [
    "generate brain connectivity maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9712b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list_train = []\n",
    "for id in ids_split[\"train\"]:\n",
    "    #print(id)\n",
    "    corr = np.corrcoef(roi[id].transpose())\n",
    "    corr = np.expand_dims(corr, axis=0) \n",
    "    corr_list_train.append(corr)    \n",
    "corr_all_train = np.concatenate(corr_list_train, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35b0c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list_val = []\n",
    "for id in ids_split[\"val\"]:\n",
    "    #print(id)\n",
    "    corr = np.corrcoef(roi[id].transpose())\n",
    "    corr = np.expand_dims(corr, axis=0) \n",
    "    corr_list_val.append(corr)    \n",
    "corr_all_val = np.concatenate(corr_list_val, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94d6fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list_test = []\n",
    "for id in ids_split[\"test\"]:\n",
    "    #print(id)\n",
    "    corr = np.corrcoef(roi[id].transpose())\n",
    "    corr = np.expand_dims(corr, axis=0) \n",
    "    corr_list_test.append(corr)    \n",
    "corr_all_test = np.concatenate(corr_list_test, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1698ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids_intersection)\n",
    "corr_list_all = []\n",
    "for id in ids_intersection:\n",
    "    #print(id)\n",
    "    corr = np.corrcoef(roi[id].transpose())\n",
    "    corr = np.expand_dims(corr, axis=0) \n",
    "    corr_list_all.append(corr)    \n",
    "corr_all_all = np.concatenate(corr_list_all, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b3bf47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('con_mat_train.npy', 'wb') as f:\n",
    "    np.save(f, corr_all_train)\n",
    "with open('con_mat_val.npy', 'wb') as f:\n",
    "    np.save(f, corr_all_val)\n",
    "with open('con_mat_test.npy', 'wb') as f:\n",
    "    np.save(f, corr_all_test)\n",
    "with open('con_mat_all.npy', 'wb') as f:\n",
    "    np.save(f, corr_all_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56ec1c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 116, 116)\n"
     ]
    }
   ],
   "source": [
    "with open('con_mat_test.npy', 'rb') as f:\n",
    "    a = np.load(f)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb85e15",
   "metadata": {},
   "source": [
    "# constructure of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e79e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_5292/2442007266.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5292/2442007266.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    (conv1): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1), padding=(6, 6))\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "  (encoder): ConvEncoder(\n",
    "    (conv1): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1), padding=(6, 6))\n",
    "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv_z): Conv2d(256, 20, kernel_size=(8, 8), stride=(1, 1))\n",
    "    (act): ReLU(inplace=True)\n",
    "  )\n",
    "  (decoder): ConvDecoder(\n",
    "    (conv1): ConvTranspose2d(10, 256, kernel_size=(8, 8), stride=(1, 1))\n",
    "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv5): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (bn5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (conv_final): ConvTranspose2d(64, 1, kernel_size=(1, 1), stride=(1, 1), padding=(6, 6))\n",
    "    (act): ReLU(inplace=True)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99685b1",
   "metadata": {},
   "source": [
    "The size of input and output is 1 * 116 * 116"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
